{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from numpy import median\n",
    "import urllib\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import string\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labeled data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseLabeledData(path):\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.split(',')\n",
    "            yield({\"asin\":line[0],\n",
    "                 \"question\":line[1],\n",
    "                 \"review\":line[2],\n",
    "                 \"answer\":line[3],\n",
    "                 \"relevance\":float(line[4])\n",
    "                })\n",
    "        \n",
    "\n",
    "print(\"Reading labeled data...\")\n",
    "data = list(parseLabeledData(\"C:/Users/Moi/Downloads/highestReviewData.csv\"))\n",
    "#data = parseLabeledData(\"/Users/Silvia/Desktop/New Data - Sheet1.csv\")\n",
    "asins = [d['asin'] for d in data]\n",
    "queries = [d['question'] for d in data]\n",
    "reviews = [d['review'] for d in data]\n",
    "answers = [d['answer'] for d in data]\n",
    "relevances = [d['relevance'] for d in data]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading all reviews & all questions...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseAllQueries(path):\n",
    "    file = open(path, 'r')\n",
    "    dataList = defaultdict(lambda: [])\n",
    "    for line in file:\n",
    "        line = eval(line)\n",
    "        dataList[line[\"asin\"]].append(line)\n",
    "      \n",
    "    return dataList\n",
    "\n",
    "def parseAllReviews(path):\n",
    "    file = open(path, 'r')\n",
    "    dataList = defaultdict(lambda: [])\n",
    "    for line in file:\n",
    "        line = eval(line)\n",
    "        dataList[line[\"asin\"]].append(line)\n",
    "      \n",
    "    return dataList\n",
    "\n",
    "print(\"Reading all reviews & all questions...\")\n",
    "\n",
    "allReviews = parseAllReviews(\"C:/Users/Moi/Downloads/reviews.json\")\n",
    "allQuestions = parseAllQueries(\"C:/Users/Moi/Downloads/qa.json\")\n",
    "\n",
    "#allReviews = parseAllReviews(\"/Users/Silvia/Downloads/reviews.json\")\n",
    "#allQuestions = parseAllQueries(\"/Users/Silvia/Downloads/qa.json\")\n",
    "\n",
    "# do we have to remove questions that have no reviews or reviews that have no questions??\n",
    "docSet = []\n",
    "for entry in allReviews.values():\n",
    "    for review in entry:\n",
    "        docSet.append(review[\"reviewText\"])\n",
    "\n",
    "for entry in allQuestions.values():\n",
    "    for question in entry:\n",
    "        docSet.append(question[\"question\"])\n",
    "\n",
    "docLen = [len(d.split()) for d in docSet]\n",
    "avgdl = sum(docLen) / len(docLen)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAllWords():\n",
    "    allWords = defaultdict(int)\n",
    "    englishStopWords = stopwords.words('english')\n",
    "    for r in allReviews.values():\n",
    "        for review in r:\n",
    "            review = review[\"reviewText\"]\n",
    "            exclude = set(string.punctuation)\n",
    "            review = ''.join(ch for ch in review if ch not in exclude)\n",
    "            for w in review.lower().split():\n",
    "                if w not in englishStopWords:\n",
    "                    allWords[w] += 1\n",
    "\n",
    "    for q in allQuestions.values():\n",
    "        for question in q:\n",
    "            question = question[\"question\"]\n",
    "            exclude = set(string.punctuation)\n",
    "            question = ''.join(ch for ch in question if ch not in exclude)\n",
    "            for w in question.lower().split():\n",
    "                if w not in englishStopWords:\n",
    "                    allWords[w] += 1\n",
    "    \n",
    "    \n",
    "    return allWords\n",
    "\n",
    "allWords = countAllWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonWords = sorted(allWords, key=lambda x: -allWords[x])[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToIndex(term):\n",
    "    if term in commonWords:\n",
    "        return commonWords.index(term)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagCache = {}\n",
    "\n",
    "def bagOfWords(document, length):\n",
    "    if (document, length) in bagCache:\n",
    "        return bagCache[(document, length)]\n",
    "    \n",
    "    bag = [0]*length\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    doc = ''.join(ch for ch in document if ch not in exclude)\n",
    "    doc = doc.lower().split()\n",
    "    \n",
    "    for term in doc:\n",
    "        index = wordToIndex(term)\n",
    "        \n",
    "        if index >= 0 and index < length:\n",
    "            bag[index] = doc.count(term)\n",
    "            \n",
    "    bagCache[(document, length)] = bag\n",
    "    \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwiseProduct(review, question, length):\n",
    "    reviewBag = bagOfWords(review, length)\n",
    "    questionBag = bagOfWords(question, length)\n",
    "        \n",
    "    bagFeat = [0]*length\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        bagFeat[i] = reviewBag[i] * questionBag[i]\n",
    "        \n",
    "    #for i in range(0, length):\n",
    "        #if reviewBag[i] > 0 or questionBag[i] > 0:\n",
    "            #print(commonWords[i], reviewBag[i], questionBag[i], bagFeat[i])\n",
    "        \n",
    "    return bagFeat        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featForOnlyQuestion(question, length):\n",
    "    return ([1]+bagOfWords(question, length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featForQuestionAndRelevantReview(review, question, length = 1000):\n",
    "    feat = [1]\n",
    "    feat += pairwiseProduct(review, question, length)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(featList):\n",
    "    \n",
    "    max = 0\n",
    "    min = float('inf')\n",
    "    for feat in featList:\n",
    "        if feat > max: max = feat\n",
    "        if feat < min: min = feat        \n",
    "    \n",
    "    for i in range(0,len(featList)-1):\n",
    "        if (max - min) == 0: \n",
    "            max = 1\n",
    "            min = 0\n",
    "        featList[i] = (featList[i] - min) / (max - min)\n",
    "\n",
    "    return featList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y): \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(y, y_hat):\n",
    "    #print(sklearn.metrics.r2_score(y, y_hat))\n",
    "    \n",
    "    \n",
    "    accuracy = sklearn.metrics.accuracy_score(y, y_hat)\n",
    "    precision = sklearn.metrics.precision_score(y, y_hat)\n",
    "    recall = sklearn.metrics.recall_score(y, y_hat)\n",
    "    \n",
    "    return \"{0:.2f}, {1:.2f}, {2:.2f}\".format(accuracy, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain(elems, point):    \n",
    "    for elem in elems:\n",
    "        if (elem > point): yield 1\n",
    "        else: yield 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73796 73796 74760 74760\n"
     ]
    }
   ],
   "source": [
    "random.seed(505)\n",
    "\n",
    "thresh = 0.0\n",
    "\n",
    "points = []\n",
    "\n",
    "i = 0\n",
    "qNum = 0\n",
    "while i < len(data):\n",
    "    \n",
    "    qReviews = []\n",
    "    qRelevances = []\n",
    "\n",
    "    question = data[i][\"question\"]\n",
    "    answer = data[i][\"answer\"]\n",
    "\n",
    "    while (i < len(data) and data[i][\"question\"] == question):        \n",
    "        if (data[i][\"relevance\"] > thresh):\n",
    "            qReviews += [data[i][\"review\"]]\n",
    "            qRelevances += [data[i][\"relevance\"]]\n",
    "        i += 1\n",
    "\n",
    "    if (len(qReviews) > 0):\n",
    "        for qReview, qRelevance in zip(qReviews, qRelevances):\n",
    "            points += [(\n",
    "                featForOnlyQuestion(question, 500) + featForQuestionAndRelevantReview(qReview, question, 500), \n",
    "                1 if answer == \"Y\" else 0,\n",
    "                qNum\n",
    "            )]\n",
    "    qNum += 1\n",
    "\n",
    "\n",
    "keys = list(range(0, starti))\n",
    "random.shuffle(keys)\n",
    "\n",
    "X_train = [p[0] for p in points if p[2] % 2 == 0]\n",
    "y_train = [p[1] for p in points if p[2] % 2 == 0]\n",
    "\n",
    "X_test = [p[0] for p in points if p[2] % 2 == 1]\n",
    "y_test = [p[1] for p in points if p[2] % 2 == 1]\n",
    "\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6655\n"
     ]
    }
   ],
   "source": [
    "random.seed(505)\n",
    "\n",
    "thresh = 0.3\n",
    "\n",
    "yesPoint = [(featForQuestionAndRelevantReview(d[\"review\"], d[\"question\"], 500), 1 if d[\"answer\"] == \"Y\" else 0, d[\"relevance\"])\n",
    "            for d, i in zip(data, range(0, len(data))) if d[\"answer\"] == \"Y\" and d[\"relevance\"] > thresh]\n",
    "noPoint  = [(featForQuestionAndRelevantReview(d[\"review\"], d[\"question\"], 500), 1 if d[\"answer\"] == \"Y\" else 0, d[\"relevance\"])\n",
    "            for d, i in zip(data, range(0, len(data))) if d[\"answer\"] == \"N\" and d[\"relevance\"] > thresh]\n",
    "\n",
    "#yesPoint = [(featForOnlyQuestion(d[\"question\"], 1000), 1 if d[\"answer\"] == \"Y\" else 0)\n",
    "#            for d in data if d[\"answer\"] == \"Y\" and d[\"relevance\"] > thresh]\n",
    "#noPoint  = [(featForOnlyQuestion(d[\"question\"], 1000), 1 if d[\"answer\"] == \"Y\" else 0)\n",
    "#            for d in data if d[\"answer\"] == \"N\" and d[\"relevance\"] > thresh]\n",
    "\n",
    "#yesPoint = [(featForOnlyQuestion(d[\"question\"], 2000)\n",
    "#             + [d[\"relevance\"]],\n",
    "#             1 if d[\"answer\"] == \"Y\" else 0)\n",
    "#            for d in data if d[\"answer\"] == \"Y\" and d[\"relevance\"] > thresh]\n",
    "#noPoint  = [(featForOnlyQuestion(d[\"question\"], 2000)\n",
    "#             + [d[\"relevance\"]],\n",
    "#             1 if d[\"answer\"] == \"Y\" else 0)\n",
    "#            for d in data if d[\"answer\"] == \"N\" and d[\"relevance\"] > thresh]\n",
    "\n",
    "#yesPoint = random.sample(yesPoint, len(noPoint))\n",
    "\n",
    "points = yesPoint + noPoint\n",
    "random.shuffle(points)\n",
    "\n",
    "X = [p[0] for p in points]\n",
    "y = [p[1] for p in points]\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4436 4436 2219 2219\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:len(X)*2//3]\n",
    "y_train = y[:len(X)*2//3]\n",
    "\n",
    "X_test = X[len(X)*2//3:]\n",
    "y_test = y[len(X)*2//3:]\n",
    "\n",
    "#X_test = [featForQuestionAndRelevantReview(d[\"review\"], d[\"question\"]) for d in data]\n",
    "#y_test = [1 if d[\"answer\"] == \"Y\" else 0 for d in data]\n",
    "#i_test = list(range(0, len(data)))\n",
    "                                           \n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict with one question and the most relevant review!\n",
      "0.5 :\t 0.81, 0.81, 0.99 \t 0.77, 0.79, 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict with one question and the most relevant review!\")\n",
    "\n",
    "y_hatTrain = [p[1] for p in lr.predict_proba(X_train)]\n",
    "y_hatTest  = [p[1] for p in lr.predict_proba(X_test)]\n",
    "\n",
    "#y_hatTrain = [p[1] for i, p in zip(i_train, lr.predict_proba(X_train)) if relevances[i] > thresh]\n",
    "#y_hatTest  = [p[1] for i, p in zip(i_test, lr.predict_proba(X_test))  if relevances[i] > thresh]\n",
    "\n",
    "for cutoff in range(10, 11):\n",
    "    y_hatTrain_c = list(constrain(y_hatTrain, cutoff / 20))\n",
    "    y_hatTest_c = list(constrain(y_hatTest, cutoff / 20))\n",
    "    #print(len(y_hatTest_c), len(y_hatTest), len(X_test), len(y_test))\n",
    "    train_string = test(y_train, y_hatTrain_c)\n",
    "    test_string = test(y_test, y_hatTest_c)\n",
    "\n",
    "    print(cutoff / 20, \":\\t\", train_string, \"\\t\", test_string)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5443199614567684\n",
      "0.86, 0.87, 0.99\n"
     ]
    }
   ],
   "source": [
    "confidence = [abs(y - 0.5)*2 for y in y_hatTest]\n",
    "med = numpy.median(confidence)\n",
    "\n",
    "print(med)\n",
    "\n",
    "y_hatTest_c = list(constrain(y_hatTest, cutoff / 20))\n",
    "\n",
    "y_test2 = [y for y, y_hat in zip(y_test, y_hatTest) if abs(y_hat-0.5)*2 > med]\n",
    "y_hatTest_c2 = [y_c for y_c, y_hat in zip(y_hatTest_c, y_hatTest) if abs(y_hat-0.5)*2 > med]\n",
    "\n",
    "print(test(y_test2, y_hatTest_c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featForQuestionAndReviews(reviews, relevances, question, length):    \n",
    "    feat = [1]\n",
    "    \n",
    "    reviewBag = [0]*length\n",
    "    \n",
    "    relevances = [r for r in relevances]\n",
    "    \n",
    "    #totalRelevance = sum(relevances)\n",
    "    \n",
    "    med = numpy.median(relevances)\n",
    "    \n",
    "    thresh = 0.5\n",
    "    count = 0\n",
    "    for (review, relevance) in zip(reviews, relevances):\n",
    "        if relevance >= thresh or relevance >= med:\n",
    "            count += 1\n",
    "    \n",
    "    for (review, relevance) in zip(reviews, relevances):\n",
    "        if (relevance >= thresh or relevance >= med):\n",
    "            bow = bagOfWords(review, length)\n",
    "            reviewBag = [x + y * (relevance) / count for x, y in zip(reviewBag, bow)]\n",
    "            \n",
    "\n",
    "    questionBag = bagOfWords(question, length)\n",
    "    \n",
    "    bagFeat = [0]*length\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        bagFeat[i] = questionBag[i] * reviewBag[i]\n",
    "        \n",
    "    cos = sum(bagFeat)\n",
    "    \n",
    "\n",
    "    feat += questionBag\n",
    "    feat += [cos]\n",
    "    feat += reviewBag\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8280\n"
     ]
    }
   ],
   "source": [
    "random.seed(505)\n",
    "\n",
    "thresh = 0.0\n",
    "\n",
    "points = []\n",
    "\n",
    "i = 0\n",
    "while i < len(data):\n",
    "    \n",
    "    qReviews = []\n",
    "    qRelevances = []\n",
    "\n",
    "    question = data[i][\"question\"]\n",
    "    answer = data[i][\"answer\"]\n",
    "    \n",
    "    while (i < len(data) and data[i][\"question\"] == question):        \n",
    "        if (data[i][\"relevance\"] > thresh):\n",
    "            qReviews += [data[i][\"review\"]]\n",
    "            qRelevances += [data[i][\"relevance\"]]\n",
    "        i += 1\n",
    "    \n",
    "    if (len(qReviews) > 0):\n",
    "        featureVector = [(featForQuestionAndReviews(qReviews, qRelevances, question, 200), 1 if answer == \"Y\" else 0)]\n",
    "        #for _ in range(0, len(qReviews)):\n",
    "        points += featureVector\n",
    "\n",
    "#yesPoint = [(featForQuestionAndReviews(d[\"review\"], d[\"question\"]), 1 if d[\"answer\"] == \"Y\" else 0, i)\n",
    "#            for d, i in zip(data, range(0, len(data))) if d[\"answer\"] == \"Y\" and d[\"relevance\"] > 0.5]\n",
    "#noPoint  = [(featForQuestionAndReviews(d[\"review\"], d[\"question\"]), 1 if d[\"answer\"] == \"Y\" else 0, i)\n",
    "#            for d, i in zip(data, range(0, len(data))) if d[\"answer\"] == \"N\" and d[\"relevance\"] > 0.5]\n",
    "\n",
    "#yesPoint = random.sample(yesPoint, len(noPoint))\n",
    "\n",
    "random.shuffle(points)\n",
    "\n",
    "X = [p[0] for p in points]\n",
    "y = [p[1] for p in points]\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4140 4140 4140 4140\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:len(X)//2]\n",
    "y_train = y[:len(X)//2]\n",
    "\n",
    "X_test = X[len(X)//2:]\n",
    "y_test = y[len(X)//2:]\n",
    "\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict with one question and all the most relevant review!\n",
      "0.5 :\t 0.70, 0.71, 0.97 \t 0.67, 0.69, 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict with one question and all the most relevant review!\")\n",
    "\n",
    "y_hatTrain = [p[1] for p in lr.predict_proba(X_train)]\n",
    "y_hatTest  = [p[1] for p in lr.predict_proba(X_test)]\n",
    "\n",
    "for cutoff in range(10, 11):\n",
    "    y_hatTrain_c = list(constrain(y_hatTrain, cutoff / 20))\n",
    "    y_hatTest_c = list(constrain(y_hatTest, cutoff / 20))\n",
    "    train_string = test(y_train, y_hatTrain_c)\n",
    "    test_string = test(y_test, y_hatTest_c)\n",
    "\n",
    "    print(cutoff / 20, \":\\t\", train_string, \"\\t\", test_string)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3765919257884106\n",
      "0.74, 0.75, 0.99\n",
      "0.79, 0.80, 0.99\n"
     ]
    }
   ],
   "source": [
    "confidence = [abs(y - 0.5)*2 for y in y_hatTest]\n",
    "med = numpy.median(confidence)\n",
    "\n",
    "print(med)\n",
    "\n",
    "y_hatTest_c = list(constrain(y_hatTest, cutoff / 20))\n",
    "\n",
    "y_test2 = [y for y, y_hat in zip(y_test, y_hatTest) if abs(y_hat-0.5)*2 > med]\n",
    "y_hatTest_c2 = [y_c for y_c, y_hat in zip(y_hatTest_c, y_hatTest) if abs(y_hat-0.5)*2 > med]\n",
    "\n",
    "print(test(y_test2, y_hatTest_c2))\n",
    "\n",
    "y_test2 = [y for y, y_hat in zip(y_test, y_hatTest) if abs(y_hat-0.5)*2 > .9]\n",
    "y_hatTest_c2 = [y_c for y_c, y_hat in zip(y_hatTest_c, y_hatTest) if abs(y_hat-0.5)*2 > .9]\n",
    "\n",
    "print(test(y_test2, y_hatTest_c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
