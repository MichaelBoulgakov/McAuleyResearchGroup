{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import urllib\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import string\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labeled data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseLabeledData(path):\n",
    "    file=open(path, 'r')\n",
    "    dataList = []\n",
    "    for line in csv.reader(file):        \n",
    "        if len(line) >= 5:\n",
    "            dataList.append(\n",
    "                {\"asin\":line[0], \n",
    "                 \"question\":line[1],\n",
    "                 \"review\":line[2],\n",
    "                 \"answer\":line[3],\n",
    "                 \"label\":line[4]}\n",
    "            )     \n",
    "    return dataList\n",
    "        \n",
    "\n",
    "print(\"Reading labeled data...\")\n",
    "#data = parseLabeledData(\"C:/Users/Moi/Downloads/out.csv\")\n",
    "data = parseLabeledData(\"/Users/Silvia/Desktop/New Data - Sheet1.csv\")\n",
    "queries = [d['question'] for d in data]\n",
    "answers = [d['answer'] for d in data]\n",
    "reviews = [d['review'] for d in data]\n",
    "labels = [d['label'] for d in data]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading all reviews & all questions...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseAllQueries(path):\n",
    "    file = open(path, 'r')\n",
    "    dataList = defaultdict(lambda: [])\n",
    "    for line in file:\n",
    "        line = eval(line)\n",
    "        dataList[line[\"asin\"]].append(line)\n",
    "      \n",
    "    return dataList\n",
    "\n",
    "def parseAllReviews(path):\n",
    "    file = open(path, 'r')\n",
    "    dataList = defaultdict(lambda: [])\n",
    "    for line in file:\n",
    "        line = eval(line)\n",
    "        dataList[line[\"asin\"]].append(line)\n",
    "      \n",
    "    return dataList\n",
    "\n",
    "print(\"Reading all reviews & all questions...\")\n",
    "\n",
    "#allReviews = parseAllReviews(\"C:/Users/Moi/Downloads/reviews.json\")\n",
    "#allQuestions = parseAllQueries(\"C:/Users/Moi/Downloads/qa.json\")\n",
    "\n",
    "allReviews = parseAllReviews(\"/Users/Silvia/Downloads/reviews.json\")\n",
    "allQuestions = parseAllQueries(\"/Users/Silvia/Downloads/qa.json\")\n",
    "\n",
    "# do we have to remove questions that have no reviews or reviews that have no questions??\n",
    "docSet = []\n",
    "for entry in allReviews.values():\n",
    "    for review in entry:\n",
    "        docSet.append(review[\"reviewText\"])\n",
    "\n",
    "for entry in allQuestions.values():\n",
    "    for question in entry:\n",
    "        docSet.append(question[\"question\"])\n",
    "\n",
    "docLen = [len(d.split()) for d in docSet]\n",
    "avgdl = sum(docLen) / len(docLen)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAllWords():\n",
    "    allWords = defaultdict(int)\n",
    "    englishStopWords = stopwords.words('english')\n",
    "    for r in allReviews.values():\n",
    "        for review in r:\n",
    "            review = review[\"reviewText\"]\n",
    "            exclude = set(string.punctuation)\n",
    "            review = ''.join(ch for ch in review if ch not in exclude)\n",
    "            for w in review.lower().split():\n",
    "                if w not in englishStopWords:\n",
    "                    allWords[w] += 1\n",
    "\n",
    "    for q in allQuestions.values():\n",
    "        for question in q:\n",
    "            question = question[\"question\"]\n",
    "            exclude = set(string.punctuation)\n",
    "            question = ''.join(ch for ch in question if ch not in exclude)\n",
    "            for w in question.lower().split():\n",
    "                if w not in englishStopWords:\n",
    "                    allWords[w] += 1\n",
    "    \n",
    "    \n",
    "    return allWords\n",
    "\n",
    "allWords = countAllWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonWords = sorted(allWords, key=lambda x: -allWords[x])[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whos\n"
     ]
    }
   ],
   "source": [
    "def wordToIndex(term):\n",
    "    if term in commonWords:\n",
    "        return commonWords.index(term)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwiseProduct(review, question, length):\n",
    "    reviewBag = [0]*length\n",
    "    questionBag = [0]*length\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    review = ''.join(ch for ch in review if ch not in exclude)\n",
    "    question = ''.join(ch for ch in question if ch not in exclude)\n",
    "    \n",
    "    for term in review.lower().split():\n",
    "        index = wordToIndex(term)\n",
    "        \n",
    "        if index >= 0 and index < length:\n",
    "            reviewBag[index] = review.lower().split().count(term)\n",
    "        \n",
    "    for term in question.lower().split():\n",
    "        index = wordToIndex(term)\n",
    "        \n",
    "        if index >= 0 and index < length:\n",
    "            questionBag[index] = question.lower().split().count(term)\n",
    "        \n",
    "    bagFeat = [0]*length\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        bagFeat[i] = reviewBag[i] * questionBag[i]\n",
    "        \n",
    "    #for i in range(0, length):\n",
    "        #if reviewBag[i] > 0 or questionBag[i] > 0:\n",
    "            #print(commonWords[i], reviewBag[i], questionBag[i], bagFeat[i])\n",
    "        \n",
    "    return bagFeat        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theLavFun(review, question):\n",
    "    feat = [1]\n",
    "    feat.append(pairwiseProduct(review, question, 5000))\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d44aa34daf90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeatList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mfeatList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "def train(X, y): \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    return lr\n",
    "\n",
    "\n",
    "X = [theLavFun(d[\"review\"], d[\"question\"]) for d in data]\n",
    "y = [1 if a == \"Y\" else 0 for a in answers]\n",
    "\n",
    "random.seed(4410)\n",
    "\n",
    "for j in range(0,len(X[0])):\n",
    "    featList = []\n",
    "    for i in range(0,len(X)-1):\n",
    "        featList.append(X[i][j])\n",
    "    featList = normalize(featList)\n",
    "    for i in range(0,len(X)-1):\n",
    "        X[i][j] = featList[i]\n",
    "\n",
    "keys = list(range(1, len(labels)))\n",
    "points = dict(zip(keys, zip(X, y)))\n",
    "random.shuffle(keys)\n",
    "X_rand = [points[key][0] for key in keys]\n",
    "y_rand = [points[key][1] for key in keys]\n",
    "\n",
    "X_train = X[:len(X_rand)*2//3]\n",
    "y_train = y[:len(y_rand)*2//3]\n",
    "\n",
    "X_test = X[len(X_rand)*2//3:]\n",
    "y_test = y[len(y_rand)*2//3:]\n",
    "\n",
    "train(X_train, y_train)\n",
    "\n",
    "y_hatTrain = lr.predict(X_train)\n",
    "y_hatTest = lr.predict(X_test)\n",
    "\n",
    "#we done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
